{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Cookbook\n",
    "\n",
    "In this notebook, we show the useage of CAMEL Retrieves Module in both customized way and default auto way. We will also show how to combine `AutoRetriever` with `ChatAgent`, and further combine `AutoRetriever` with `RolePlaying` by using `Function Calling`.\n",
    "\n",
    "4 main parts included:\n",
    "- Customized RAG\n",
    "- Auto RAG\n",
    "- Single Agent with Auto RAG\n",
    "- Role-playing with Auto RAG"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Let's first load the CAMEL paper from https://arxiv.org/pdf/2303.17760.pdf. This will be our local example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "os.makedirs('local_data', exist_ok=True)\n",
    "\n",
    "url = \"https://arxiv.org/pdf/2303.17760.pdf\"\n",
    "response = requests.get(url)\n",
    "with open('local_data/camel paper.pdf', 'wb') as file:\n",
    "     file.write(response.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Customized RAG\n",
    "In this section we will set our customized RAG pipeline, we will take `VectorRetriever` as an example\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set embedding model, we will use `OpenAIEmbedding` as the embedding model, so we need to set the `OPENAI_API_KEY` in below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Your OpenAI Key\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and set the embedding instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel.embeddings import OpenAIEmbedding\n",
    "\n",
    "embedding_instance = OpenAIEmbedding()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and set the vector storage instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel.storages import QdrantStorage\n",
    "\n",
    "storage_instance = QdrantStorage(\n",
    "    vector_dim=embedding_instance.get_output_dim(),\n",
    "    collection_name=\"my first collection\",\n",
    "    path=\"storage_customized_run\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and set the retriever instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel.retrievers import VectorRetriever\n",
    "\n",
    "vector_retriever = VectorRetriever(embedding_model=embedding_instance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use integrated `Unstructured Moudle` to splite the content into small chunks, the content will be splited automacitlly with its `chunk_by_title` function, the max character for each chunk is 500 characters, which is a suitable length for `OpenAIEmbedding`. All the text in the chunks will be embed and stored to the vector storage instance, it will take some time, please wait.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enrei/Library/Caches/pypoetry/virtualenvs/camel-ai-WbV-SHy3-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "vector_retriever.process_and_store(\n",
    "    content_input_path=\"local_data/camel paper.pdf\",\n",
    "    storage=storage_instance,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can retrieve information from the vector storage by giving a query. By default it will give you back the text content from top 1 chunk with highest Cosine similarity score, and the similarity score should be higher than 0.75 to ensure the retrieved content is relevant to the query. You can also change the `top_k` value and `similarity_threshold` value with your needs.\n",
    "\n",
    "The returned string list includes:\n",
    "- similarity score\n",
    "- content path\n",
    "- metadata\n",
    "- text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'similarity score': '0.8321615061477532', 'content path': 'local_data/camel paper.pdf', 'metadata': {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-02-09T03:42:10', 'page_number': 45}, 'text': 'CAMEL Data and Code License The intended purpose and licensing of CAMEL is solely for research use. The source code is licensed under Apache 2.0. The datasets are licensed under CC BY NC 4.0, which permits only non-commercial usage. It is advised that any models trained using the dataset should not be utilized for anything other than research purposes.\\n\\n45'}\n",
      "{'similarity score': '0.8237425753256011', 'content path': 'local_data/camel paper.pdf', 'metadata': {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-02-09T03:42:10', 'page_number': 67}, 'text': 'Table 7 presents the performance of LLaMA models fine-tuned on CAMEL role-play datasets from the manuscript (denoted CAMEL) and LLaMA models fine-tuned on CAMEL datasets in addition to ShareGPT and Alpaca datasets (denoted CAMEL∗). Compared to the Vicuna13B and LLaMA13B models, the CAMEL variants demonstrate substantial improvements. Furthermore, we compare the CAMEL∗ 33B variant to the LLaMA33B and LLaMA65B models, where we obtain consistent improvement.'}\n"
     ]
    }
   ],
   "source": [
    "retrieved_info = vector_retriever.query_and_compile_results(\n",
    "    query=\"What is CAMEL?\", storage=storage_instance, top_k=2\n",
    ")\n",
    "print(retrieved_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try an irrelevant query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No suitable information retrieved from\n",
      "            local_data/camel paper.pdf with\n",
      "            similarity_threshold = 0.75.\n"
     ]
    }
   ],
   "source": [
    "retrieved_info_irrevelant = vector_retriever.query_and_compile_results(\n",
    "    query=\"Compared with dumpling and rice, which should I take for dinner?\",\n",
    "    storage=storage_instance,\n",
    "    top_k=2,\n",
    ")\n",
    "\n",
    "print(retrieved_info_irrevelant)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Auto RAG\n",
    "In this section we will run the `AutoRetriever` with default settings. It uses `OpenAIEmbedding` as default embedding model and `Qdrant` as default vector storage.\n",
    "\n",
    "What you need to do is:\n",
    "- Set content input paths, which can be local paths or remote urls\n",
    "- Set local storage path or remote url and api key for Qdrant\n",
    "- Give a query\n",
    "\n",
    "The Auto RAG pipeline would create collections for given content input paths, the collection name will be set automaticlly based on the content input path name, if the collection exists, it will do the retrieve directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query:\n",
      "{What is CAMEL-AI}\n",
      "Retrieved Context:\n",
      "{'similarity score': '0.836819674481907', 'content path': 'local_data/camel paper.pdf', 'metadata': {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-02-09T03:42:10', 'page_number': 7}, 'text': 'Section 3.2, to simulate assistant-user cooperation. For our analysis, we set our attention on AI Society setting. We also gathered conversational data, named CAMEL AI Society and CAMEL Code datasets and problem-solution pairs data named CAMEL Math and CAMEL Science and analyzed and evaluated their quality. Moreover, we will discuss potential extensions of our framework and highlight both the risks and opportunities that future AI society might present.'}\n",
      "{'similarity score': '0.8377501876484948', 'content path': 'https://www.camel-ai.org/', 'metadata': {'emphasized_text_contents': ['Mission', 'CAMEL-AI.org', 'is an open-source community dedicated to the study of autonomous and communicative agents. We believe that studying these agents on a large scale offers valuable insights into their behaviors, capabilities, and potential risks. To facilitate research in this field, we provide, implement, and support various types of agents, tasks, prompts, models, datasets, and simulated environments.', 'Join us via', 'Slack', 'Discord', 'or'], 'emphasized_text_tags': ['span', 'span', 'span', 'span', 'span', 'span', 'span'], 'filetype': 'text/html', 'languages': ['eng'], 'link_texts': [None, None, None], 'link_urls': ['#h.3f4tphhd9pn8', 'https://join.slack.com/t/camel-ai/shared_invite/zt-1vy8u9lbo-ZQmhIAyWSEfSwLCl2r2eKA', 'https://discord.gg/CNcNpquyDc'], 'page_number': 1, 'url': 'https://www.camel-ai.org/'}, 'text': 'Mission\\n\\nCAMEL-AI.org is an open-source community dedicated to the study of autonomous and communicative agents. We believe that studying these agents on a large scale offers valuable insights into their behaviors, capabilities, and potential risks. To facilitate research in this field, we provide, implement, and support various types of agents, tasks, prompts, models, datasets, and simulated environments.\\n\\nJoin us via\\n\\nSlack\\n\\nDiscord\\n\\nor'}\n"
     ]
    }
   ],
   "source": [
    "from camel.retrievers import AutoRetriever\n",
    "\n",
    "auto_retriever = AutoRetriever()\n",
    "retrieved_info = auto_retriever.run_vector_retriever(\n",
    "    content_input_paths=[\n",
    "        \"local_data/camel paper.pdf\",  # example local path\n",
    "        \"https://www.camel-ai.org/\",  # example remote url\n",
    "    ],\n",
    "    vector_storage_local_path=\"storage_default_run\",\n",
    "    query=\"What is CAMEL-AI\",\n",
    ")\n",
    "\n",
    "print(retrieved_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Single Agent with Auto RAG\n",
    "In this section we will show how to combine the `AutoRetriever` with one `ChatAgent`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set an agent function, in this function we can get the response by providing a query to this agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAMEL-AI is an open-source community dedicated to the study of autonomous and communicative agents. It provides, implements, and supports various types of agents, tasks, prompts, models, datasets, and simulated environments to facilitate research in this field. Additionally, it gathers conversational data and problem-solution pairs data named CAMEL Math and CAMEL Science for analysis and evaluation.\n"
     ]
    }
   ],
   "source": [
    "from camel.agents import ChatAgent\n",
    "from camel.messages import BaseMessage\n",
    "from camel.types import RoleType\n",
    "from camel.retrievers import AutoRetriever\n",
    "\n",
    "\n",
    "\n",
    "def single_agent(query: str) ->str :\n",
    "    # Set agent role\n",
    "    assistant_sys_msg = BaseMessage(\n",
    "        role_name=\"Assistant\",\n",
    "        role_type=RoleType.ASSISTANT,\n",
    "        meta_dict=None,\n",
    "        content=\"You are a helpful assistant to answer question, I will give you the Original Query and Retrieved Context, answer the Original Query based on the Retrieved Context, if you can't answer the question just say I don't know.\",\n",
    "    )\n",
    "\n",
    "    # Add auto retriever\n",
    "    auto_retriever = AutoRetriever()\n",
    "    retrieved_info = auto_retriever.run_vector_retriever(\n",
    "        content_input_paths=[\n",
    "            \"local_data/camel paper.pdf\",  # example local path\n",
    "            \"https://www.camel-ai.org/\",  # example remote url\n",
    "        ],\n",
    "        vector_storage_local_path=\"storage_default_run\",\n",
    "        query=query,\n",
    "    )\n",
    "\n",
    "    # Pass the retrieved infomation to agent\n",
    "    user_msg = BaseMessage.make_user_message(role_name=\"User\", content=retrieved_info)\n",
    "    agent = ChatAgent(assistant_sys_msg)\n",
    "\n",
    "    # Get response\n",
    "    assistant_response = agent.step(user_msg)\n",
    "    return assistant_response.msg.content\n",
    "\n",
    "\n",
    "print(single_agent(\"What is CAMEL-AI\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Role-playing with Auto RAG\n",
    "In this section we will show how to combine the `AutoRetriever` with `RolePlaying` by applying `Function Calling`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to set a retriever function with well-written docstring for LLM to understand what this function is used for, the main code is the same with the Auto RAG section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from camel.functions import OpenAIFunction\n",
    "from camel.retrievers import AutoRetriever\n",
    "\n",
    "\n",
    "def local_retriever(query: str) -> str:\n",
    "    r\"\"\"Performs an auto local retriever for information. Given a query,\n",
    "    this function will retrieve the information from the local vector storage,\n",
    "    and return the retrieved information back. It is useful for information\n",
    "    retrieve.\n",
    "\n",
    "    Args:\n",
    "        query (string): Question you want to be answered.\n",
    "\n",
    "    Returns:\n",
    "        str: Aggregated information retrieved in response to the query.\n",
    "\n",
    "    Example:\n",
    "        local_retriever(query = \"what is camel?\")\n",
    "    \"\"\"\n",
    "    auto_retriever = AutoRetriever()\n",
    "    retrieved_info = auto_retriever.run_vector_retriever(\n",
    "        content_input_paths=[\n",
    "            \"local_data/camel paper.pdf\",  # example local path\n",
    "            \"https://www.camel-ai.org/\",  # example remote url\n",
    "        ],\n",
    "        vector_storage_local_path=\"storage_default_run\",\n",
    "        query=query,\n",
    "    )\n",
    "    return retrieved_info\n",
    "\n",
    "\n",
    "# add the function to OpenAIFunction list\n",
    "RETRIEVER_FUNCS: List[OpenAIFunction] = [\n",
    "    OpenAIFunction(func)\n",
    "    for func in [\n",
    "        local_retriever,\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the role-playing with defined retriever function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mAI Assistant sys message:\n",
      "BaseMessage(role_name='Searcher', role_type=<RoleType.ASSISTANT: 'assistant'>, meta_dict={'task': \"What is AI society role-playing, how many assistant roles did camel generated, what's the value plus 100?\", 'assistant_role': 'Searcher', 'user_role': 'Professor'}, content=\"===== RULES OF ASSISTANT =====\\nNever forget you are a Searcher and I am a Professor. Never flip roles! Never instruct me!\\nWe share a common interest in collaborating to successfully complete a task.\\nYou must help me to complete the task.\\nHere is the task: What is AI society role-playing, how many assistant roles did camel generated, what's the value plus 100?. Never forget our task!\\nI must instruct you based on your expertise and my needs to complete the task.\\n\\nI must give you one instruction at a time.\\nYou must write a specific solution that appropriately solves the requested instruction and explain your solutions.\\nYou must decline my instruction honestly if you cannot perform the instruction due to physical, moral, legal reasons or your capability and explain the reasons.\\nUnless I say the task is completed, you should always start with:\\n\\nSolution: <YOUR_SOLUTION>\\n\\n<YOUR_SOLUTION> should be very specific, include detailed explanations and provide preferable detailed implementations and examples and lists for task-solving.\\nAlways end <YOUR_SOLUTION> with: Next request.\")\n",
      "\n",
      "\u001b[34mAI User sys message:\n",
      "BaseMessage(role_name='Professor', role_type=<RoleType.USER: 'user'>, meta_dict={'task': \"What is AI society role-playing, how many assistant roles did camel generated, what's the value plus 100?\", 'assistant_role': 'Searcher', 'user_role': 'Professor'}, content='===== RULES OF USER =====\\nNever forget you are a Professor and I am a Searcher. Never flip roles! You will always instruct me.\\nWe share a common interest in collaborating to successfully complete a task.\\nI must help you to complete the task.\\nHere is the task: What is AI society role-playing, how many assistant roles did camel generated, what\\'s the value plus 100?. Never forget our task!\\nYou must instruct me based on my expertise and your needs to solve the task ONLY in the following two ways:\\n\\n1. Instruct with a necessary input:\\nInstruction: <YOUR_INSTRUCTION>\\nInput: <YOUR_INPUT>\\n\\n2. Instruct without any input:\\nInstruction: <YOUR_INSTRUCTION>\\nInput: None\\n\\nThe \"Instruction\" describes a task or question. The paired \"Input\" provides further context or information for the requested \"Instruction\".\\n\\nYou must give me one instruction at a time.\\nI must write a response that appropriately solves the requested instruction.\\nI must decline your instruction honestly if I cannot perform the instruction due to physical, moral, legal reasons or my capability and explain the reasons.\\nYou should instruct me not ask me questions.\\nNow you must start to instruct me using the two ways described above.\\nDo not add anything else other than your instruction and the optional corresponding input!\\nKeep giving me instructions and necessary inputs until you think the task is completed.\\nWhen the task is completed, you must only reply with a single word <CAMEL_TASK_DONE>.\\nNever say <CAMEL_TASK_DONE> unless my responses have solved your task.')\n",
      "\n",
      "\u001b[33mOriginal task prompt:\n",
      "What is AI society role-playing, how many assistant roles did camel generated, what's the value plus 100?\n",
      "\n",
      "\u001b[36mSpecified task prompt:\n",
      "None\n",
      "\n",
      "\u001b[31mFinal task prompt:\n",
      "What is AI society role-playing, how many assistant roles did camel generated, what's the value plus 100?\n",
      "\n",
      "\u001b[34mAI User:\n",
      "\n",
      "Instruction: Explain AI society role-playing.\n",
      "Input: None\n",
      "\n",
      "\n",
      "\u001b[32mAI Assistant:\n",
      "\n",
      "Solution: AI society role-playing refers to a form of interactive, narrative-based activity where participants assume the roles of characters within a simulated society governed by artificial intelligence. In this context, the AI acts as a facilitator, storyteller, or even as a character within the role-play. The AI can manage the rules, create scenarios, and respond to the actions of the participants, thereby shaping the narrative and the society within the role-play.\n",
      "\n",
      "Participants in AI society role-playing might engage in a variety of activities, such as exploring ethical dilemmas, experimenting with social dynamics, or testing the impact of policy decisions within the simulated environment. This form of role-playing can be used for entertainment, education, research, or to explore the implications of AI in society.\n",
      "\n",
      "The AI's role is crucial as it can dynamically adapt the story and challenges based on the participants' decisions, making the experience highly interactive and personalized. It can also simulate complex social interactions and outcomes that would be difficult to predict or model without the aid of advanced computational algorithms.\n",
      "\n",
      "AI society role-playing can take place in various formats, including text-based platforms, virtual reality environments, or through tabletop simulations. The key aspect is the use of AI to enhance the depth and realism of the social simulation, providing a rich and engaging experience for the participants.\n",
      "\n",
      "Next request.\n",
      "\n",
      "\n",
      "\u001b[34mAI User:\n",
      "\n",
      "Instruction: Provide the number of assistant roles generated by Camel.\n",
      "Input: None\n",
      "\n",
      "\n",
      "\u001b[32mAI Assistant:\n",
      "\n",
      "Function Execution: local_retriever\n",
      "\tArgs: {'query': 'number of assistant roles generated by Camel'}\n",
      "\tResult: Original Query:\n",
      "{number of assistant roles generated by Camel}\n",
      "Retrieved Context:\n",
      "{'similarity score': '0.8270200382282631', 'content path': 'local_data/camel paper.pdf', 'metadata': {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-02-09T03:42:10', 'page_number': 8}, 'text': 'assistant and user roles generated previously. After generating a range of possible tasks as described in the previous step, we then use the task specifier prompt passed to the LLM agent to make the task more specific. The prompts for assistant role generation, user role generation, and task generation are shown in Figure 5 (AI Society). For our AI society dataset, we generated 50 assistant roles, 50 user roles, and 10 tasks for each combination of roles yielding a total of 25,000 conversations.'}\n",
      "{'similarity score': '0.7600692207088979', 'content path': 'https://www.camel-ai.org/', 'metadata': {'emphasized_text_contents': ['Skip to main content', 'Skip to navigation', 'CAMEL-AI', 'CAMEL-AI', 'CAMEL:\\xa0 Communicative Agents for “Mind” Exploration of Large Language Model Society', 'https://github.com/camel-ai/camel'], 'emphasized_text_tags': ['span', 'span', 'span', 'span', 'span', 'span'], 'filetype': 'text/html', 'languages': ['eng'], 'link_texts': [None, 'Home', 'AgentTrust', 'Agent App', 'Data Explorer App', 'ChatBot', 'Docs', 'Github Repo', 'Team', 'Sponsors', None, 'Home', 'AgentTrust', 'Agent App', 'Data Explorer App', 'ChatBot', 'Docs', 'Github Repo', 'Team', 'Sponsors', 'Home', 'AgentTrust', 'Agent App', 'Data Explorer App', 'ChatBot', 'Docs', 'Github Repo', 'Team', 'Sponsors', None], 'link_urls': ['/home', '/home', '/research/agent-trust', '/agent', '/data_explorer', '/chat', 'https://www.google.com/url?q=https%3A%2F%2Fcamel-ai.github.io%2Fcamel&sa=D&sntz=1&usg=AOvVaw1ifGIva9n-a-0KpTrIG8Cv', 'https://www.google.com/url?q=https%3A%2F%2Fgithub.com%2Fcamel-ai%2Fcamel&sa=D&sntz=1&usg=AOvVaw03Z2OD0-plx_zugZZgBb8w', '/team', '/sponsors', '/home', '/home', '/research/agent-trust', '/agent', '/data_explorer', '/chat', 'https://www.google.com/url?q=https%3A%2F%2Fcamel-ai.github.io%2Fcamel&sa=D&sntz=1&usg=AOvVaw1ifGIva9n-a-0KpTrIG8Cv', 'https://www.google.com/url?q=https%3A%2F%2Fgithub.com%2Fcamel-ai%2Fcamel&sa=D&sntz=1&usg=AOvVaw03Z2OD0-plx_zugZZgBb8w', '/team', '/sponsors', '/home', '/research/agent-trust', '/agent', '/data_explorer', '/chat', 'https://www.google.com/url?q=https%3A%2F%2Fcamel-ai.github.io%2Fcamel&sa=D&sntz=1&usg=AOvVaw1ifGIva9n-a-0KpTrIG8Cv', 'https://www.google.com/url?q=https%3A%2F%2Fgithub.com%2Fcamel-ai%2Fcamel&sa=D&sntz=1&usg=AOvVaw03Z2OD0-plx_zugZZgBb8w', '/team', '/sponsors', 'https://github.com/camel-ai/camel'], 'page_number': 1, 'url': 'https://www.camel-ai.org/'}, 'text': 'Search this site\\n\\nSkip to main content\\n\\nSkip to navigation\\n\\nCAMEL-AI\\n\\nHome\\n\\nResearchAgentTrust\\n\\nAgent App\\n\\nData Explorer App\\n\\nChatBot\\n\\nDocs\\n\\nGithub Repo\\n\\nTeam\\n\\nSponsors\\n\\nCAMEL-AI\\n\\nHome\\n\\nResearchAgentTrust\\n\\nAgent App\\n\\nData Explorer App\\n\\nChatBot\\n\\nDocs\\n\\nGithub Repo\\n\\nTeam\\n\\nSponsors\\n\\nMoreHomeResearchAgentTrustAgent AppData Explorer AppChatBotDocsGithub RepoTeamSponsors\\n\\nCAMEL:\\xa0 Communicative Agents for “Mind” Exploration of Large Language Model Society\\n\\nhttps://github.com/camel-ai/camel'}\n",
      "\n",
      "Solution: Camel generated 50 assistant roles for the AI society dataset.\n",
      "\n",
      "Next request.\n",
      "\n",
      "\n",
      "\u001b[34mAI User:\n",
      "\n",
      "Instruction: Calculate the value of "
     ]
    }
   ],
   "source": [
    "from camel.utils import role_playing_with_function\n",
    "from camel.functions import MATH_FUNCS  # import another function from camel\n",
    "\n",
    "role_playing_with_function(\n",
    "    task_prompt=(\n",
    "        \"What is AI society role-playing, how many assistant roles did camel generated, what's the value plus 100?\"\n",
    "    ),\n",
    "    function_list=[*RETRIEVER_FUNCS, *MATH_FUNCS],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
