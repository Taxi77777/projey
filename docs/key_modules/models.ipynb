{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üê´ CAMEL Model Setting Tutorial\n",
        "\n"
      ],
      "metadata": {
        "id": "dhi3bjORSnqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also check this cookbook in colab [here](https://colab.research.google.com/drive/18hQLpte6WW2Ja3Yfj09NRiVY-6S2MFu7?usp=sharing)"
      ],
      "metadata": {
        "id": "XFyl4eJue3Sm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *TLDR*"
      ],
      "metadata": {
        "id": "-nCRZU5sInDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CAMEL enables flexible integration of various AI models, acting as the brain of intelligent agents. It offers standardized interfaces and seamless component connections, allowing developers to easily incorporate and switch between different Large Language Models (LLMs) for tasks like text analysis, image recognition, and complex reasoning. This versatility empowers the creation of adaptable AI applications that can leverage the right model for each specific task."
      ],
      "metadata": {
        "id": "C5ZY0xD2aGyg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Content:\n",
        "* Introduction\n",
        "* Supported Model Platforms\n",
        "* Model Calling Template\n",
        "* How to Connect to Open Source LLMs\n",
        "* Conclusion"
      ],
      "metadata": {
        "id": "oPJNz-SUZ16T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "The model is the brain of the intelligent agent, responsible for processing all input and output data. By calling different models, the agent can execute operations such as text analysis, image recognition, or complex reasoning according to task requirements. CAMEL offers a range of standard and customizable interfaces, as well as seamless integrations with various components, to facilitate the development of applications with Large Language Models (LLMs). In this blog, we will introduce models currently supported by CAMEL and the working principles and interaction methods with models.All the codes are also available on colab notebook."
      ],
      "metadata": {
        "id": "GvAvfIBuTLkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supported Model Platforms\n",
        "\n",
        "The following table lists currently supported model platforms by CAMEL."
      ],
      "metadata": {
        "id": "mSwHM9_RZRsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model Platform | Representative Model| Multi-modality |\n",
        "| ----- | ----- | ----- |\n",
        "| OpenAI | gpt-4o | Y |\n",
        "| Azure OpenAI | gpt-4o | Y |\n",
        "| Mistral AI | mistral-large-2 | N |\n",
        "| Anthropic | claude-3-5-sonnet-20240620 | Y |\n",
        "| Gemini | gemini-1.5-pro | Y |\n",
        "| ZhipuAI | glm-4v | Y |\n",
        "| Reka | reka-core | Y |\n",
        "| Nividia | nemotron-4-340b-reward | N |\n",
        "| SambaNova| https://sambaverse.sambanova.ai/models | ----- |\n",
        "| Groq | https://console.groq.com/docs/models | ----- |\n",
        "| Ollama | https://ollama.com/library | ----- |\n",
        "| vLLM | https://docs.vllm.ai/en/latest/models/supported_models.html | ----- |\n",
        "| Together AI | https://docs.together.ai/docs/chat-models | ----- |\n",
        "| LiteLLM | https://docs.litellm.ai/docs/providers | ----- |"
      ],
      "metadata": {
        "id": "kuLESB0rZTEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model calling template\n",
        "\n",
        "Here is the example code to use a chosen model. To utilize a different model, you can simply change three parameters the define your model to be used: `model_platform`, `model_type`, `model_config_dict` .\n"
      ],
      "metadata": {
        "id": "QNJbeFeKTlwO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation"
      ],
      "metadata": {
        "id": "XuZwbB1IIuUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensure you have CAMEL AI installed in your Python environment:"
      ],
      "metadata": {
        "id": "gIJK4Lu5Iwm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install camel-ai==0.1.9"
      ],
      "metadata": {
        "id": "HTKnWg9Xv_y4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from camel.models import ModelFactory\n",
        "from camel.types import ModelPlatformType, ModelType\n",
        "from camel.configs import ChatGPTConfig\n",
        "from camel.messages import BaseMessage\n",
        "from camel.agents import ChatAgent\n",
        "\n",
        "# Define the model, here in this case we use gpt-4o-mini\n",
        "model = ModelFactory.create(\n",
        "    model_platform=ModelPlatformType.OPENAI,\n",
        "    model_type=ModelType.GPT_4O_MINI,\n",
        "    model_config_dict=ChatGPTConfig().as_dict,\n",
        ")\n",
        "\n",
        "# Define an assitant message\n",
        "system_msg = BaseMessage.make_assistant_message(\n",
        "        role_name=\"Assistant\",\n",
        "        content=\"You are a helpful assistant.\",\n",
        ")\n",
        "\n",
        "# Initialize the agent\n",
        "ChatAgent(system_msg, model=model)"
      ],
      "metadata": {
        "id": "5VCSeMm2TN3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Open source LLMs\n",
        "In the current landscape, for those seeking highly stable content generation, OpenAI‚Äôs gpt-4o-mini, gpt-4o are often recommended. However, the field is rich with many other outstanding open-source models that also yield commendable results. CAMEL can support developers to delve into integrating these open-source large language models (LLMs) to achieve project outputs based on unique input ideas.\n",
        "\n",
        "While proprietary models like gpt-4o-mini and gpt-4o have set high standards for content generation, open-source alternatives offer viable solutions for experimentation and practical use. These models, supported by active communities and continuous improvements, provide flexibility and cost-effectiveness for developers and researchers.\n",
        "We will walk you through the process of:\n",
        "\n",
        "1. **Selecting an Open Source LLM**: Understand the strengths and capabilities of various models available in the open-source community.\n",
        "2. **Setting Up the Environment**: Learn how to set up your development environment to integrate these models. This includes installing necessary libraries, configuring runtime environments, and ensuring compatibility with your project requirements.\n",
        "3. **Model Integration**: Step-by-step guidance on integrating your chosen LLM into your development workflow. We will cover API configurations to maximize model performance for tasks.\n",
        "\n",
        "While CAMEL implements some of models in-house, others are integrated through third-party providers. This hybrid approach enables CAMEL to provide a comprehensive and flexible toolkit for building with LLMs.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2zxPicpWTt3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use Open-Source Models as Backends (ex. using Ollama to set Llama 3 locally)\n",
        "\n"
      ],
      "metadata": {
        "id": "fZmSoz07T3Iw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Download [Ollama](https://ollama.com/download).\n",
        "2. After setting up Ollama, pull the Llama3 model by typing the following command into the terminal:"
      ],
      "metadata": {
        "id": "JgEnTDD8UJvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ollama pull llama3"
      ],
      "metadata": {
        "id": "XHJz6nLfT0zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create a `ModelFile` similar the one below in your project directory.\n"
      ],
      "metadata": {
        "id": "YoAGInAKUO-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FROM llama3\n",
        "\n",
        "# Set parameters\n",
        "PARAMETER temperature 0.8\n",
        "PARAMETER stop Result\n",
        "\n",
        "# Sets a custom system message to specify the behavior of the chat assistant\n",
        "# Leaving it blank for now.\n",
        "\n",
        "SYSTEM \"\"\" \"\"\""
      ],
      "metadata": {
        "id": "lHWLCdCIUNkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Create a script to get the base model (llama3) and create a custom model using the `ModelFile` above. Save this as a `.sh` file:"
      ],
      "metadata": {
        "id": "GA09d4s2Uefx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/bin/zsh\n",
        "\n",
        "# variables\n",
        "model_name=\"llama3\"\n",
        "custom_model_name=\"camel-llama3\"\n",
        "\n",
        "#get the base model\n",
        "ollama pull $model_name\n",
        "\n",
        "#create the model file\n",
        "ollama create $custom_model_name -f ./Llama3ModelFile"
      ],
      "metadata": {
        "id": "qKd-VwK2Ueqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Navigate to the directory where the script and `ModelFile` are located and run the script. Enjoy your Llama3 model, enhanced by CAMEL's excellent agents.\n"
      ],
      "metadata": {
        "id": "dFOvO1jjUn59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from camel.agents import ChatAgent\n",
        "from camel.messages import BaseMessage\n",
        "from camel.models import ModelFactory\n",
        "from camel.types import ModelPlatformType\n",
        "\n",
        "ollama_model = ModelFactory.create(\n",
        "    model_platform=ModelPlatformType.OLLAMA,\n",
        "    model_type=\"llama3\",\n",
        "    url=\"http://localhost:11434/v1\",\n",
        "    model_config_dict={\"temperature\": 0.4},\n",
        ")\n",
        "\n",
        "assistant_sys_msg = BaseMessage.make_assistant_message(\n",
        "    role_name=\"Assistant\",\n",
        "    content=\"You are a helpful assistant.\",\n",
        ")\n",
        "agent = ChatAgent(assistant_sys_msg, model=ollama_model, token_limit=4096)\n",
        "\n",
        "user_msg = BaseMessage.make_user_message(\n",
        "    role_name=\"User\", content=\"Say hi to CAMEL\"\n",
        ")\n",
        "assistant_response = agent.step(user_msg)\n",
        "print(assistant_response.msg.content)"
      ],
      "metadata": {
        "id": "j_Io1-GEUoL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use Open-Source Models as Backends (ex. using vLLM to set Phi-3 locally)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ip6ubb5sUobL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install [vLLM](https://docs.vllm.ai/en/latest/getting_started/installation.html) first."
      ],
      "metadata": {
        "id": "13vW949qU1Ag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After setting up vLLM, start an OpenAI compatible server for example by:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "suSDajsWU63r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python -m vllm.entrypoints.openai.api_server --model microsoft/Phi-3-mini-4k-instruct --api-key vllm --dtype bfloat16"
      ],
      "metadata": {
        "id": "XtGf5wElU7AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create and run following script (more details please refer to this [example](https://github.com/camel-ai/camel/blob/master/examples/models/vllm_model_example.py)):\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HKQiPucTU7Ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from camel.agents import ChatAgent\n",
        "from camel.messages import BaseMessage\n",
        "from camel.models import ModelFactory\n",
        "from camel.types import ModelPlatformType\n",
        "\n",
        "vllm_model = ModelFactory.create(\n",
        "    model_platform=ModelPlatformType.VLLM,\n",
        "    model_type=\"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    url=\"http://localhost:8000/v1\",\n",
        "    model_config_dict={\"temperature\": 0.0},\n",
        "    api_key=\"vllm\",\n",
        ")\n",
        "\n",
        "assistant_sys_msg = BaseMessage.make_assistant_message(\n",
        "    role_name=\"Assistant\",\n",
        "    content=\"You are a helpful assistant.\",\n",
        ")\n",
        "agent = ChatAgent(assistant_sys_msg, model=vllm_model, token_limit=4096)\n",
        "\n",
        "user_msg = BaseMessage.make_user_message(\n",
        "    role_name=\"User\",\n",
        "    content=\"Say hi to CAMEL AI\",\n",
        ")\n",
        "assistant_response = agent.step(user_msg)\n",
        "print(assistant_response.msg.content)"
      ],
      "metadata": {
        "id": "fP-NvbNBU7PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "In conclusion, CAMEL empowers developers to explore and integrate these diverse models, unlocking new possibilities for innovative AI applications. The world of large language models offers a rich tapestry of options beyond just the well-known proprietary solutions. By guiding users through model selection, environment setup, and integration, CAMEL bridges the gap between cutting-edge AI research and practical implementation. Its hybrid approach, combining in-house implementations with third-party integrations, offers unparalleled flexibility and comprehensive support for LLM-based development. Don't just watch this transformation that is happening from the sidelines.\n",
        "\n",
        "Dive into the CAMEL documentation, experiment with different models, and be part of shaping the future of AI. The era of truly flexible and powerful AI is here - are you ready to make your mark?\n"
      ],
      "metadata": {
        "id": "4x27BVoKVHCb"
      }
    }
  ]
}